{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage\n",
    "from IPython.display import display\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_samples = 20, cols = 4):\n",
    "    \n",
    "    plt.figure(figsize = (15, 15))\n",
    "    \n",
    "    for i, img in enumerate(data):\n",
    "        if i == num_samples:\n",
    "            break\n",
    "        plt.subplot(int(num_samples / cols + 1), cols, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[0])\n",
    "    \n",
    "data = torchvision.datasets.CIFAR10(root = './cifar-10', download=True)\n",
    "show_images(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta scheduling\n",
    "\n",
    "def linear_beta_schedule(timesteps, start = 0.0001, end = 0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, start = 0.0001, end = 0.02):\n",
    "    betas = torch.linspace(0, 1, timesteps)\n",
    "    betas = 0.5 * (1 + torch.cos(betas * math.pi))\n",
    "    betas = start + (end - start) * betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timesteps T & beta \n",
    "\n",
    "T = 100\n",
    "# betas = linear_beta_schedule(timesteps=T)\n",
    "betas = cosine_beta_schedule(timesteps=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 1 - betas # alpha & beta\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0) # \\Pi alpha = alpha x alpha ...\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value = 1) # timestep t cumprod alpha = timestep {t-1} cumprod alpha x t step alpha\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas) # 1 / alpha = noise의 variance\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod) # alphas_cumprod: noise 크기의 누적 변화 (\\Pi alpha)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod) # root 1-cumprod alpha\n",
    "posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod) # noise variance에 따른 posterior variance. reconstruction에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "특정 timestep t에서의 값을 가져오기 위한 장치\n",
    "vals: timestep에 다라 변하는 값을 가진 리스트 (2차원)\n",
    "t: 특정 timestep의 시간을 나타내는 tensor (2차원)\n",
    "'''\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0] # t는 embedding된 timestep의 tensor\n",
    "    out = vals.gather(-1, t.cpu())  # t에 해당하는 값을 가져옴.\n",
    "    \n",
    "    return out.reshape(batch_size, *((1, ) * (len(x_shape) - 1))).to(t.device) # out tensor의 형태를 조정. (batch_size, 1,1,1,...,1) 로 바꿔서 x_0와 동일한 형태\n",
    "\n",
    "\n",
    "# x_0와 timestep T를 input으로 받고 noise로 만들기\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \n",
    "    noise = torch.randn_like(x_0)  \n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def load_transformed_dataset():\n",
    "    datat_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # scale between [-1, 1]\n",
    "    ]\n",
    "    \n",
    "    data_transforms = transforms.Compose(datat_transforms)\n",
    "    \n",
    "    # train & test for cifar-10\n",
    "    train = torchvision.datasets.CIFAR10(root='./cifar-10', download=True, train=True, transform=data_transforms)\n",
    "    \n",
    "    test = torchvision.datasets.CIFAR10(root='./cifar-10', download=True, train=False, transform=data_transforms)\n",
    "    \n",
    "    \n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "        transforms.Lambda(lambda t: t * 255.0),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    \n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "data = load_transformed_dataset()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Process\n",
    "$$ p_\\theta(x_T) = \\mathcal{N}(x_t; 0, I)$$\n",
    "$$ p_\\theta(x_{0:T}) = p(x_T) \\Pi^T_{t=1} p_\\theta(x_{t-1}|x_t)$$\n",
    "$$ x_{t-1} \\approx x_t - noise $$\n",
    "\n",
    "## Unet\n",
    "- input: noisy image\n",
    "- timestep encodding: Sinusodial embeddings\n",
    "- variance is fixed\n",
    "\n",
    "## Timestep Encoding\n",
    "- 각 timestep의 위치 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SinusodialPositionalEncoding\n",
    "\n",
    "class SinusodialPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, timesteps):\n",
    "        device = timesteps.device\n",
    "        half_dim = self.dim // 2 # 절반은 sin, 절반은 cos\n",
    "        embeddings = math.log(10000) / (half_dim - 1) # \n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings) # 1/10000^(2i/d)\n",
    "        embeddings = timesteps[:, None] * embeddings[None, :] # t/10000^(2i/d)\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1) # sin(t/10000^(2i/d)), cos(t/10000^(2i/d))\n",
    "        \n",
    "        return embeddings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up = False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        \n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        \n",
    "        else: \n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, t, ):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimension\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = [64, 128, 256, 512, 1024]\n",
    "        up_channels = [1024, 512, 256, 128, 64]\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32\n",
    "        \n",
    "        # Time Embeddings\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusodialPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, 1)\n",
    "        \n",
    "        # Downsample\n",
    "        self.donws = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels) - 1)])\n",
    "        \n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels) - 1)])\n",
    "        \n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "        \n",
    "    def forward (self, x, timesteps):\n",
    "        # Embedding time\n",
    "        t = self.time_mlp(timesteps)\n",
    "        # initial projection\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.donws:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "    \n",
    "model = SimpleUnet()\n",
    "print(\"Num parameters:\", sum([p.numel() for p in model.parameters()]))\n",
    "model\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    \n",
    "    return F.l2_loss(noise_pred, noise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        # As pointed out by Luis Pereira (see YouTube comment)\n",
    "        # The t's are offset from the t's in the paper\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        # Edit: This is to maintain the natural range of the distribution\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize)+1)\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr = 0.0001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device = device).long()\n",
    "        loss = get_loss(model, batch[0], t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: {}, Step: {}, Loss: {}\".format(epoch, step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
